# Predict command
## COMMAND: [ python, predict.py ]
# Train command
TRAIN_COMMAND: [ python, train.py ]
LOO_CV_COMMAND: [python, loo_cross_validation.py]

CLASSES:
  - { label: 81,  name: "Wake",       }
  - { label: 82,  name: "N1" ,        }
  - { label: 83,  name: "N2",        replace: 82}
  - { label: 84,  name: "N3",        replace: 82}
  - { label: 85,  name: "REM",       replace: 82}
  - { label: 86,  name: "Movement",  replace: 82}

# --- Information about data
# Path to training dataset 
TRAIN_DATA: /path/to/DualSleep

# Amount of training data used for validation (between 0 and 1)
VALID_SPLIT: test

# List of subjects to use for LOSO testing
TEST_SUBJECTS: null
SEED: 42

NUM_WORKERS: 5  # Num of workers for data loading
NUM_GPUS: [1]  # How many GPUs to use
WANDB: False
WANDB_KEY: '<Put wandb key here>'


DATASET: STFT
DATASET_ARGS:
  x_columns: [[back_x, back_y, back_z, thigh_x, thigh_y, thigh_z]]
  y_column: [label]
  padding_val: [0.0]
  # 50==1sec ==> 3000==60sec==1min
  sequence_length: [5_400_000]  # Window size (in samples) fed into the model, here 30hours
  frame_shift: [null]  # How much to shift a window (in samples) null=same as sequence_length
  normalize: [True]  # Whether to normalize the training data
  # Use existing normalization params:
  norm_params_path: [params/norm_params/HUNT4/normalization_params_HUNT4_feats9006_seqlen3599/]
  n_fft: [3_000]  # Size of Fourier transform (in samples), here 1min
  phase: [False]
  hop_length: [null]  # For STFT computation. If None, n_fft//3
  # Kind determines how labels in each window should be processed
  # density is distribution in n_fft-sized window
  windowed_labels_kind: [density]  # Allowed: 'counts', 'density', 'onehot', 'argmax', None
  # Timestamps are 5-dim int vectors of the form: [month, day, weekday, hour, minute]
  return_timestamps: [False]
  timestamp_column: [timestamp]

# -- Model 
# Which classifier to use 
ALGORITHM: DownstreamMLP
# Arguments for classifier
# (all given as lists in case to perform a GridSearch)
ALGORITHM_ARGS:
  epochs: [60]
  batch_size: [1]
  loss: [CrossEntropyLoss]
  optimizer: [Adam]
  weight_decay: [0.0]
  output_activation: [softmax]
  metrics: [[KLD]]  # Which metrics to log
  lr: [2e-4]
  lr_scheduler: [ExponentialTFDecay]
  # Architecture params
  upstream_algorithm: [TransformerEncoderNetwork]
  upstream_model_path: [params/LTA2V_upstream/upstream_model.ckpt]
  rmv_upstream_layers: [4]  # remove the last 4 layers: 3metrics+1pred heads
  rmv_GPE: [True]  # Don't use GPE during downstream
  weighted_sum_layer: [null]  # None=no weighted sum
  fine_tune_step: [0.75]
  dropout: [0.1]  # Dropout probability
  n_prediction_head_layers: [2]
  dim_prediction_head: [1028]  # Dim of hidden layers of MLP at end od model


STORE_CMATS: True  # Store test cmats on disk
SKIP_FINISHED_ARGS: False

# Metric which defines the best model
EVAL_METRIC: average_MAE
# Additional metrics to compute
ADDITIONAL_EVAL_METRICS: [KLD, MAE, average_MAE]
# In case aggregation is required for a metric (e.g., KLD or MAE)
METRIC_AGGR_WINDOW_LEN: 3000
SAVE_PREDICTIONS: True
